{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6vQ+V8u5A4ocFoaIghaZf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthikeya210/PERSONAL/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StgfsP3gp72j",
        "outputId": "475fa48f-89a0-4d0d-e104-b74060e4f1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-353fafd39da7>:8: DtypeWarning: Columns (1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('alphabets_28x28.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(251135, 785)\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 5s 6ms/step - loss: 0.8876 - accuracy: 0.8909 - val_loss: 0.1436 - val_accuracy: 0.9580\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 6s 9ms/step - loss: 0.0960 - accuracy: 0.9716 - val_loss: 0.1013 - val_accuracy: 0.9702\n",
            "Epoch 3/10\n",
            "647/647 [==============================] - 4s 6ms/step - loss: 0.0650 - accuracy: 0.9806 - val_loss: 0.0904 - val_accuracy: 0.9744\n",
            "Epoch 4/10\n",
            "647/647 [==============================] - 3s 5ms/step - loss: 0.0518 - accuracy: 0.9838 - val_loss: 0.0872 - val_accuracy: 0.9747\n",
            "Epoch 5/10\n",
            "647/647 [==============================] - 4s 6ms/step - loss: 0.0412 - accuracy: 0.9872 - val_loss: 0.0774 - val_accuracy: 0.9783\n",
            "Epoch 6/10\n",
            "647/647 [==============================] - 5s 8ms/step - loss: 0.0342 - accuracy: 0.9890 - val_loss: 0.0854 - val_accuracy: 0.9778\n",
            "Epoch 7/10\n",
            "647/647 [==============================] - 3s 5ms/step - loss: 0.0348 - accuracy: 0.9886 - val_loss: 0.0694 - val_accuracy: 0.9822\n",
            "Epoch 8/10\n",
            "647/647 [==============================] - 4s 6ms/step - loss: 0.0276 - accuracy: 0.9910 - val_loss: 0.0927 - val_accuracy: 0.9781\n",
            "Epoch 9/10\n",
            "647/647 [==============================] - 4s 6ms/step - loss: 0.0265 - accuracy: 0.9909 - val_loss: 0.0880 - val_accuracy: 0.9795\n",
            "Epoch 10/10\n",
            "647/647 [==============================] - 5s 8ms/step - loss: 0.0257 - accuracy: 0.9916 - val_loss: 0.0738 - val_accuracy: 0.9826\n",
            "647/647 [==============================] - 2s 3ms/step\n",
            "Accuracy on test set: 0.9826419108403442\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('alphabets_28x28.csv')\n",
        "print(df.shape)\n",
        "\n",
        "df_filtered = df.groupby('label').head(6000)\n",
        "\n",
        "df_filtered = df_filtered.dropna()\n",
        "\n",
        "X = df_filtered.iloc[:, 1:].values.astype(np.float32)\n",
        "y = df_filtered.iloc[:, 0].values.astype(str)\n",
        "\n",
        "X = X.reshape(X.shape[0], 28, 28, 1)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "y = to_categorical(y, num_classes=26)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "ocr_model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(26, activation='softmax')\n",
        "])\n",
        "\n",
        "ocr_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "ocr_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n",
        "\n",
        "predictions = ocr_model.predict(X_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = np.mean(predicted_labels == true_labels)\n",
        "print(\"Accuracy on test set:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp8gM9e0qvkS",
        "outputId": "484c47c4-1237-4318-c79e-b373150a251e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "data = pd.read_csv('sentiment_analysis_dataset.csv')\n",
        "\n",
        "stopset = stopwords.words('english')\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=stopset)\n",
        "\n",
        "X = data.line\n",
        "y = data.sentiment\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_vec, y_train)\n",
        "\n",
        "y_pred_proba = clf.predict_proba(X_test_vec)\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "print(f'ROC AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "random_sentence = \"I am so happy to be a core member of Epoch\"\n",
        "\n",
        "random_sentence_vec = vectorizer.transform([random_sentence])\n",
        "\n",
        "predicted_sentiment = clf.predict(random_sentence_vec)[0]\n",
        "\n",
        "print(f\"Random Sentence: {random_sentence}\")\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgOGCA95qwRd",
        "outputId": "860e564a-98fb-4cf8-a1fb-e3700ef54975"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC Score: 0.97\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       1.00      0.50      0.67         2\n",
            "       Happy       0.40      1.00      0.57         2\n",
            "     Neutral       1.00      0.50      0.67         4\n",
            "\n",
            "    accuracy                           0.62         8\n",
            "   macro avg       0.80      0.67      0.63         8\n",
            "weighted avg       0.85      0.62      0.64         8\n",
            "\n",
            "Random Sentence: I am so happy to be a core member of Epoch\n",
            "Predicted Sentiment: Happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRY1\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    return binary_image\n",
        "def segment_characters(binary_image):\n",
        "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    characters = []\n",
        "    for cnt in contours:\n",
        "        (x, y, w, h) = cv2.boundingRect(cnt)\n",
        "        if w > 5 and h > 5:\n",
        "            char_img = binary_image[y:y+h, x:x+w]\n",
        "            char_img = cv2.resize(char_img, (28, 28), interpolation=cv2.INTER_AREA)\n",
        "            characters.append(char_img)\n",
        "    characters = sorted(characters, key=lambda img: cv2.boundingRect(img)[0])\n",
        "    return characters\n",
        "index_to_char = {\n",
        "    0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K',\n",
        "    11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U',\n",
        "    21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z',\n",
        "    26: 'a', 27: 'b', 28: 'c', 29: 'd', 30: 'e', 31: 'f', 32: 'g', 33: 'h', 34: 'i', 35: 'j',\n",
        "    36: 'k', 37: 'l', 38: 'm', 39: 'n', 40: 'o', 41: 'p', 42: 'q', 43: 'r', 44: 's', 45: 't',\n",
        "    46: 'u', 47: 'v', 48: 'w', 49: 'x', 50: 'y', 51: 'z',\n",
        "    52: '0', 53: '1', 54: '2', 55: '3', 56: '4', 57: '5', 58: '6', 59: '7', 60: '8', 61: '9',\n",
        "    62: ' '\n",
        "}\n",
        "def recognize_characters(characters, ocr_model):\n",
        "    recognized_text = \"\"\n",
        "    for char_img in characters:\n",
        "        char_img = char_img / 255.0\n",
        "        char_img = np.expand_dims(char_img, axis=0)\n",
        "        char_img = np.expand_dims(char_img, axis=-1)\n",
        "        predicted_char_idx = ocr_model.predict(char_img).argmax(axis=-1)[0]\n",
        "        predicted_char = index_to_char[predicted_char_idx]\n",
        "        recognized_text += predicted_char\n",
        "    return recognized_text\n",
        "image_path = 'line_4.png'\n",
        "binary_image = preprocess_image(image_path)\n",
        "characters = segment_characters(binary_image)\n",
        "recognized_text = recognize_characters(characters, ocr_model)\n",
        "print(\"Recognized Text:\", recognized_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63w-xvMyrIG9",
        "outputId": "8cf25fb6-b2fe-4473-a4b5-eba979e719b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 86ms/step\n",
            "Recognized Text: P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRY2\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    return binary_image\n",
        "\n",
        "def segment_characters(binary_image):\n",
        "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    characters = []\n",
        "    for cnt in contours:\n",
        "        (x, y, w, h) = cv2.boundingRect(cnt)\n",
        "        if w > 5 and h > 5:\n",
        "            char_img = binary_image[y:y+h, x:x+w]\n",
        "            char_img = cv2.resize(char_img, (28, 28), interpolation=cv2.INTER_AREA)\n",
        "            characters.append(char_img)\n",
        "    characters = sorted(characters, key=lambda img: cv2.boundingRect(img)[0])\n",
        "    return characters\n",
        "\n",
        "def recognize_characters(characters, ocr_model, label_encoder):\n",
        "    recognized_text = \"\"\n",
        "    for char_img in characters:\n",
        "        char_img = char_img / 255.0\n",
        "        char_img = np.expand_dims(char_img, axis=0)\n",
        "        char_img = np.expand_dims(char_img, axis=-1)\n",
        "        prediction = ocr_model.predict(char_img)\n",
        "        predicted_class = np.argmax(prediction, axis=1)\n",
        "        predicted_char = label_encoder.inverse_transform(predicted_class)\n",
        "        recognized_text += predicted_char[0]\n",
        "    return recognized_text\n",
        "\n",
        "image_path = 'line_4.png'\n",
        "binary_image = preprocess_image(image_path)\n",
        "characters = segment_characters(binary_image)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(list(\"abcdefghijklmnopqrstuvwxyz\"))\n",
        "\n",
        "recognized_text = recognize_characters(characters, ocr_model, label_encoder)\n",
        "print(\"Recognized Text:\", recognized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y39NoJ7rkGl",
        "outputId": "392a01cb-c3f3-475a-d205-490bf2cf7338"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "Recognized Text: p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_hfF8hNxzZbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}